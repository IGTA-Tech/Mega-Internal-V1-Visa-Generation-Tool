# Comprehensive Research Methodology Training

**Purpose:** Professional attorney methodology for conducting Perplexity research to discover 50-70 high-quality URLs per case.

**Referenced By:** `phase1-identity-discovery.md`, `phase2-criterion-deep-dive.md`, `phase3-media-recognition.md`

---

## 1. Professional Attorney Research Methodology

### 1.1 The "5-7 Pages" Rule

**Principle:** Each quality source should provide 5-7 pages of usable content for the petition.

**What This Means:**
- Don't just find sources - find SUBSTANTIAL sources
- Brief mentions (< 100 words) don't count as full sources
- Feature articles and profiles are ideal
- Multiple brief mentions from same outlet = 1 source

**Application:**
- When counting sources, assess depth not just existence
- 10 deep sources > 50 superficial mentions
- Look for articles where beneficiary is PRIMARY subject

### 1.2 Context Capture Methodology

**Principle:** Rankings and achievements must be captured IN CONTEXT.

**Wrong Approach:**
> "John Smith is ranked #15"

**Correct Approach:**
> "John Smith is ranked #15 AMONG 2,500+ active fighters in the lightweight division worldwide, placing him in the top 0.6% of competitors globally"

**Context Elements to Capture:**
- Total pool size (ranked among how many?)
- Time period (current vs. historical)
- Geographic scope (regional/national/international)
- Selection criteria (how was ranking determined?)

### 1.3 Multiple Ranking Systems

**Principle:** Never rely on a single ranking system.

**For Each Ranked Achievement:**
- Find at least 2-3 ranking systems
- Document methodology differences
- Note if rankings agree or diverge
- Use official/governing body rankings as anchor

**Example (MMA):**
| System | Ranking | Total Ranked | Notes |
|--------|---------|--------------|-------|
| Tapology | #15 | 2,500+ | Fan-driven, comprehensive |
| Fight Matrix | #18 | 1,200+ | Algorithm-based |
| UFC Official | #12 | 15 | Promotion-specific |

### 1.4 Verification Through Contacts

**Principle:** When possible, verify claims through direct contact with organizations.

**Verification Methods:**
1. Official organization websites
2. Direct email/phone verification (document attempts)
3. Public records requests
4. LinkedIn verification (for positions)
5. Patent/trademark databases

---

## 2. Four-Tier Source Quality Framework

### 2.1 Tier 1 - Gold Standard (Target: 26+ sources)

**Definition:** Major national/international media with millions in reach.

**Examples:**
| Category | Publications |
|----------|--------------|
| General News | ESPN, BBC, CNN, NYT, WSJ, Reuters, AP, USA Today |
| Sports | Fox Sports, NBC Sports, CBS Sports, Sky Sports, Eurosport |
| Tech | TechCrunch, Wired, The Verge, Ars Technica |
| Entertainment | Variety, Hollywood Reporter, Billboard, Rolling Stone |
| Business | Forbes, Bloomberg, Fortune, Business Insider |

**Characteristics:**
- Monthly reach: 50M+ visitors
- Professional editorial standards
- Fact-checking processes
- Named journalists with credentials
- Verifiable publication dates

**USCIS Weight:** Highest evidentiary value. Single Tier 1 article with beneficiary as primary subject can satisfy Published Material criterion.

### 2.2 Tier 2 - Strong/Industry (Target: 18+ sources)

**Definition:** Official league sources, industry publications, major regional outlets.

**Examples:**
| Category | Publications |
|----------|--------------|
| Official Sports | UFC.com, NBA.com, FIFA.com, ATP, WTA, PGA |
| Industry Sports | MMA Junkie, Sherdog, Tapology, The Athletic |
| Regional Major | LA Times, Chicago Tribune, Washington Post |
| Industry Tech | Product Hunt, Hacker News, Stack Overflow |

**Characteristics:**
- Monthly reach: 1M-50M visitors
- Industry authority
- Editorial oversight
- Recognized in field

**USCIS Weight:** High. Multiple Tier 2 sources combine for strong evidence.

### 2.3 Tier 3 - Supplementary (Target: 8 max)

**Definition:** Niche publications, local media, established blogs.

**Examples:**
- Specialist blogs with editorial standards
- Local newspapers
- City magazines
- Podcasts with transcripts
- Industry newsletters

**Characteristics:**
- Monthly reach: 10K-1M visitors
- Niche audience
- May lack formal editorial process
- Variable credibility

**USCIS Weight:** Moderate. Supporting context only - never primary evidence.

### 2.4 Tier 4 - Exclude (Do NOT Use)

**Definition:** Sources that harm petition credibility.

**Always Exclude:**
- Social media posts (even verified accounts)
- Wikipedia text (use only for source discovery)
- Self-published content
- Press releases without independent coverage
- Promotional materials
- User-generated content
- Unverifiable blogs

**Why:** USCIS officers recognize these as unreliable. Including them suggests desperation or weak case.

---

## 3. Wikipedia Mining Strategy (CRITICAL)

### 3.1 Purpose

Wikipedia itself cannot be cited, but it's the BEST source discovery tool.

### 3.2 Mining Process

**Step 1: Find Wikipedia Page**
- Search for beneficiary, organization, or achievement
- Note: Not all subjects have Wikipedia pages

**Step 2: Extract External Links**
- Scroll to "External links" section at bottom
- Click EVERY link
- Verify each link works
- Document source tier for each

**Step 3: Extract References**
- Scroll to "References" section
- Click EVERY reference link
- Wikipedia references often contain 20-50 source URLs
- Many are Tier 1-2 sources

**Step 4: View Source for Hidden URLs**
- Click "View source" or "Edit"
- Search for additional URLs in markup
- Find archived versions of dead links

### 3.3 Wikipedia Mining Checklist

- [ ] Beneficiary's Wikipedia page (if exists)
- [ ] Organization's Wikipedia page
- [ ] Event/competition Wikipedia pages
- [ ] Award Wikipedia pages
- [ ] Field/sport Wikipedia page (for context sources)

### 3.4 CRITICAL RULE

**NEVER cite Wikipedia text in any petition document.**

Wikipedia is a source DISCOVERY tool only. All citations must be to the underlying sources.

---

## 4. Field-Specific Research Protocols

### 4.1 Combat Sports (MMA/Boxing/Kickboxing)

**Primary Sources:**
| Source | Type | Tier | Key Data |
|--------|------|------|----------|
| Tapology | Rankings/Records | 2 | Complete fight history, rankings |
| Fight Matrix | Rankings | 2 | Independent algorithm rankings |
| Sherdog | News/Records | 2 | Fight finder, historical data |
| MMA Junkie | News | 1* | USA Today Sports property |
| BoxRec | Boxing Records | 2 | Official boxing records |
| UFC.com | Official | 2 | UFC fighter profiles |

*MMA Junkie is Tier 1 because it's owned by USA Today Sports.

**Key Evidence to Find:**
- Win/loss record with opponent rankings
- Championship titles and defenses
- World/regional rankings over time
- National team selection (if applicable)
- Fight purses and disclosed pay

**National Team = Game Changer:**
National team selection satisfies MULTIPLE criteria:
- Awards (selection itself)
- Membership (team membership)
- Critical Role (representing country)
- Published Material (media coverage)

### 4.2 Tennis

**Primary Sources:**
| Source | Type | Tier | Key Data |
|--------|------|------|----------|
| ATP Tour | Official Men's | 2 | Rankings, results, prize money |
| WTA | Official Women's | 2 | Rankings, results, prize money |
| ITF | Federation | 2 | Davis Cup, Fed Cup, junior records |
| Tennis Explorer | Statistics | 2 | Historical rankings, H2H |
| Tennis Abstract | Analytics | 3 | Advanced statistics |

**Key Evidence to Find:**
- World ranking history and peak ranking
- Grand Slam/ATP/WTA tournament results
- National team selection (Davis Cup/Fed Cup/Billie Jean King Cup)
- Prize money earned
- H2H records vs. top players

**National Team Selection is Critical:**
Davis Cup/Fed Cup selection is among strongest evidence for tennis players.

### 4.3 Cricket

**Primary Sources:**
| Source | Type | Tier | Key Data |
|--------|------|------|----------|
| ESPN Cricinfo | Gold Standard | 1 | Complete career records, statistics |
| ICC | Official | 2 | Rankings, international records |
| National Boards | Official | 2 | Domestic records, contracts |
| Cricbuzz | News/Stats | 2 | Coverage, live scores |
| HowStat | Statistics | 3 | Historical data |

**Key Evidence to Find:**
- International caps (Test, ODI, T20)
- Career statistics (average, strike rate, etc.)
- Tournament performances
- National team selection timeline
- IPL/domestic league participation

**Country Competitiveness Matters:**
Playing for a top-tier cricket nation (India, Australia, England) carries more weight than associate members.

### 4.4 Figure Skating

**Primary Sources:**
| Source | Type | Tier | Key Data |
|--------|------|------|----------|
| ISU | Official | 2 | World rankings, competition results |
| Olympic.org | Official | 1 | Olympic participation records |
| National Federations | Official | 2 | National championship results |
| IceNetwork | Industry | 2 | US skating coverage |

**Key Evidence to Find:**
- Olympic/World Championship participation
- ISU rankings
- National championship results
- Professional skating (Disney on Ice = distinguished organization)

**Disney on Ice:**
Performing with Disney on Ice is significant evidence of Leading/Critical Role for a distinguished organization.

### 4.5 Tech/Business

**Primary Sources:**
| Source | Type | Tier | Key Data |
|--------|------|------|----------|
| TechCrunch | News | 1 | Funding, launches, profiles |
| Crunchbase | Database | 2 | Company data, funding history |
| Wired | News | 1 | Tech profiles, trends |
| GitHub | Platform | 2 | Stars, contributions, repos |
| Stack Overflow | Platform | 2 | Reputation, contributions |
| Product Hunt | Platform | 2 | Product launches, upvotes |

**Key Evidence to Find:**
- Funding rounds (amount, investors)
- Product launches and metrics
- User/revenue metrics (if public)
- Technical innovations
- Speaking engagements
- Patent applications/grants

**GitHub Metrics:**
- 1,000+ stars = significant project
- 10,000+ stars = highly significant
- Major contributions to popular repos counts

### 4.6 Music/Entertainment

**Primary Sources:**
| Source | Type | Tier | Key Data |
|--------|------|------|----------|
| Billboard | Charts | 1 | Chart positions, certifications |
| Rolling Stone | News | 1 | Reviews, profiles |
| Variety | Industry | 1 | Entertainment news |
| Pitchfork | Reviews | 1 | Album reviews, features |
| Grammy.com | Official | 2 | Award nominations/wins |
| Spotify/Apple Music | Platforms | 2 | Streaming metrics |

**Key Evidence to Find:**
- Chart positions (Billboard Hot 100, etc.)
- Album/single certifications (Gold, Platinum)
- Award nominations/wins
- Tour grosses
- Streaming numbers
- Critical reviews

---

## 5. Source Independence Verification

### 5.1 Why Independence Matters

USCIS scrutinizes whether evidence sources are truly independent. Sources that appear independent but are actually coordinated undermine petition credibility.

### 5.2 Independence Verification Checklist

For EACH pair of sources, verify:
- [ ] Different parent companies
- [ ] Different editorial teams
- [ ] Original reporting (not syndicated)
- [ ] Different time periods
- [ ] Different geographic focus
- [ ] Different angles/perspectives

### 5.3 Major Media Conglomerates

**Disney:** ESPN, ABC, FX, National Geographic
**Fox Corporation:** Fox Sports, Fox News
**Warner Bros. Discovery:** CNN, HBO, TNT Sports
**Paramount Global:** CBS, MTV, Showtime
**NBCUniversal:** NBC Sports, MSNBC, Peacock
**Hearst:** ESPN (minority), newspapers

**Rule:** Two sources from same conglomerate can still count IF:
- Different editorial teams
- Different markets
- Significant time between coverage

### 5.4 Syndication Detection

**Red Flags:**
- Identical headlines across outlets
- Same article text with different bylines
- AP/Reuters wire story reprinted
- Press release language repeated

**Action:** If syndicated, count as ONE source from highest-tier outlet.

---

## 6. Evidence Quality Scoring

### 6.1 Point Allocation by Source Tier

| Tier | Points Per Source | Max Points |
|------|-------------------|------------|
| Tier 1 | 10 points | No max |
| Tier 2 | 6 points | No max |
| Tier 3 | 3 points | 24 max (8 sources) |
| Tier 4 | 0 points | Excluded |

### 6.2 Criterion-Specific Weighting

**Awards Criterion:**
- Official award documentation: 20 points
- Major media coverage of award: 15 points
- Award ceremony coverage: 10 points
- Industry publication mention: 5 points

**Published Material Criterion:**
- Tier 1 feature article (primary subject): 20 points
- Tier 1 mention (not primary): 10 points
- Tier 2 feature article: 12 points
- Tier 2 mention: 6 points

### 6.3 Aggregate Scoring

**Strong Case:** 150+ points
**Adequate Case:** 100-149 points
**Weak Case:** 50-99 points
**Insufficient:** < 50 points

---

## 7. Verification and Anti-Hallucination Protocols

### 7.1 CRITICAL: Verify Everything

**NEVER include information that cannot be verified through accessible sources.**

### 7.2 Verification Requirements

**For EVERY URL:**
- [ ] URL is accessible (not 404)
- [ ] Content matches description
- [ ] Publication date is verifiable
- [ ] Author/source is credible
- [ ] Archive.org backup created

**For EVERY Claim:**
- [ ] Claim appears in 2+ independent sources
- [ ] Numbers/statistics match across sources
- [ ] Dates are consistent
- [ ] No contradicting information found

**For EVERY Expert:**
- [ ] Credentials independently verifiable
- [ ] No conflict of interest
- [ ] Statements within expertise

### 7.3 Red Flags for Potential Errors

**Immediately Investigate:**
- Claims appearing in only ONE source
- Statistics that seem unusually favorable
- Awards from unknown organizations
- Rankings from unrecognized systems
- Gaps in career timeline
- Inconsistent dates across sources

### 7.4 What to Do When Verification Fails

1. **Cannot verify claim:** EXCLUDE from documents
2. **Conflicting information:** Use most authoritative source, note discrepancy
3. **Source inaccessible:** Use archive.org or exclude
4. **Expert credentials unclear:** Request verification or exclude

### 7.5 Documentation Trail

For EVERY piece of evidence, maintain:
```
SOURCE: [URL]
ARCHIVED: [archive.org URL]
VERIFIED: [Date] [Method]
CROSS-REFERENCE: [Other sources confirming]
DISCREPANCIES: [Any noted, with resolution]
```

---

## 8. Research Session Best Practices

### 8.1 Query Formulation Techniques

**Name Variations:**
- Full name: "John Michael Smith"
- Common name: "John Smith"
- Nicknames: "Johnny Smith"
- Professional name: "J. Smith"
- Native script: (if applicable)

**Organization Searches:**
- "[Beneficiary Name]" + "[Organization Name]"
- "[Organization Name]" + "awards"
- "[Organization Name]" + "members"

**Event Searches:**
- "[Beneficiary Name]" + "[Event Name]"
- "[Event Name]" + "[Year]" + "results"
- "[Event Name]" + "winners"

### 8.2 Search Depth Protocol

**Phase 1 (Identity):**
- Search beneficiary name across all variations
- Search every organization mentioned
- Mine Wikipedia for all subjects

**Phase 2 (Criteria):**
- Search criterion-specific terms
- Search field-specific databases
- Search ranking systems

**Phase 3 (Media):**
- Search major media by name
- Search for interviews
- Search for profiles
- Search video/podcast content

### 8.3 When to Stop Searching

**Continue if:**
- Below URL target (50-70)
- Below tier distribution targets
- Criterion has < minimum sources

**Stop when:**
- All targets met
- Searches returning duplicate results
- Exhausted all search variations

---

## 9. Quality Checkpoints

### 9.1 After Phase 1 (Identity Discovery)

- [ ] 15-20 URLs found
- [ ] 50%+ are Tier 1-2
- [ ] Beneficiary identity confirmed
- [ ] Primary achievements documented
- [ ] Wikipedia mining complete

### 9.2 After Phase 2 (Criterion Deep Dive)

- [ ] 35-50 URLs cumulative
- [ ] Each primary criterion has 5+ sources
- [ ] Field-specific databases searched
- [ ] Rankings documented from multiple systems
- [ ] National team/selection evidence found (if applicable)

### 9.3 After Phase 3 (Media Recognition)

- [ ] 50-70 URLs total
- [ ] 65%+ Tier 1-2 overall
- [ ] Feature articles found (not just mentions)
- [ ] Media coverage spans multiple outlets
- [ ] Video/podcast content with transcripts found

### 9.4 Final Verification

- [ ] ALL URLs accessible
- [ ] ALL claims cross-referenced
- [ ] ALL archive.org backups created
- [ ] NO single-source claims included
- [ ] NO Tier 4 sources included

---

## 10. Recovery Protocols

### 10.1 Below URL Target

**If < 50 URLs after all phases:**
1. Expand search to related fields (if EXPANDING scope)
2. Search international media in other languages
3. Search older coverage (archive.org)
4. Search video platforms for transcripts
5. Search podcast databases

### 10.2 Below Tier 1-2 Ratio

**If < 65% Tier 1-2:**
1. Focus additional searches on Tier 1 outlets specifically
2. Search for feature articles vs. mentions
3. Check if Tier 3 sources have Tier 1-2 syndication
4. Document why Tier 1-2 coverage is limited

### 10.3 Criterion Gaps

**If criterion has < minimum sources:**
1. Search for alternative evidence types
2. Consider comparable evidence (O-1A, O-1B, EB-1A only)
3. Document evidence gap for strategy document
4. May need to drop criterion from petition

---

*Last Updated: December 2024*
